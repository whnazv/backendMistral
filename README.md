===========================================================
 README DEL PROYECTO ‚Äî Chat Local con Llama.cpp + Flask
===========================================================

Este archivo README est√° completamente dentro de un comentario
Java, tal como solicitaste. Puedes copiarlo tal cual.

-----------------------------------------------------------
ü¶ô 1. DESCRIPCI√ìN DEL PROYECTO
-----------------------------------------------------------

Este proyecto permite ejecutar un chat estilo ChatGPT de forma
totalmente local usando:

- llama.cpp como servidor de inferencia (puerto 8081)
- Flask como backend API (puerto 8000)
- Frontend HTML/JS para la interfaz de chat
- Modelos GGUF descargados por el usuario (no incluidos)

El backend recibe mensajes, construye un historial estilo
[INST] ... [/INST], llama al servidor llama.cpp y devuelve la
respuesta al frontend.

-----------------------------------------------------------
üìÅ 2. ESTRUCTURA DEL PROYECTO
-----------------------------------------------------------

backend/
    app.py
frontend/
    index.html
    estilo.css
modelos/
    README.txt   ‚Üê instrucciones para descargar modelos
requirements.txt
scripts/
    download_model.py (opcional)

-----------------------------------------------------------
‚öôÔ∏è 3. INSTALACI√ìN DE DEPENDENCIAS
-----------------------------------------------------------

1. Crear entorno virtual (opcional pero recomendado):

    python -m venv venv
    source venv/bin/activate   (Linux/Mac)
    venv\Scripts\activate      (Windows)

2. Instalar dependencias:

    pip install -r requirements.txt

-----------------------------------------------------------
üß† 4. MODELOS (NO INCLUIDOS)
-----------------------------------------------------------

Los modelos NO se incluyen en el repositorio debido a su tama√±o.

Debes descargarlos manualmente o mediante un script.

Modelos recomendados (GGUF):

- Mistral 7B Instruct
- Mixtral 8x7B
- Phi-2
- LLaMA 3 Instruct

Descarga desde Hugging Face:
https://huggingface.co/models?search=gguf

Coloca el archivo .gguf dentro de:

    modelos/

-----------------------------------------------------------
üîΩ 5. DESCARGA AUTOM√ÅTICA (OPCIONAL)
-----------------------------------------------------------

Si usas un script como scripts/download_model.py:

    python scripts/download_model.py

Este script descargar√° el modelo y lo guardar√° en modelos/.

-----------------------------------------------------------
‚ö° 6A. COMPILACI√ìN DE LLAMA.CPP CON SOPORTE CUDA (OPCIONAL)
-----------------------------------------------------------

Esta secci√≥n es SOLO necesaria si deseas compilar llama.cpp
manualmente con aceleraci√≥n por GPU (CUDA).

Si utilizas binarios precompilados, puedes omitir este paso.

-----------------------------------------------------------
üñ•Ô∏è REQUISITOS
-----------------------------------------------------------

- GPU NVIDIA compatible con CUDA
- Drivers NVIDIA instalados
- CUDA Toolkit
- Git, CMake, build-essential

-----------------------------------------------------------
üì¶ 1. INSTALAR DEPENDENCIAS B√ÅSICAS
-----------------------------------------------------------

    sudo apt update
    sudo apt install -y \
        git \
        cmake \
        build-essential \
        python3 \
        python3-pip

-----------------------------------------------------------
üöÄ 2. INSTALAR CUDA TOOLKIT
-----------------------------------------------------------

Verifica primero que tu GPU es compatible:
https://developer.nvidia.com/cuda-gpus

Instala los drivers NVIDIA (ejemplo):

    sudo apt install -y nvidia-driver-535
    reboot

Instala CUDA Toolkit (ejemplo):

    sudo apt install -y nvidia-cuda-toolkit

Verifica instalaci√≥n:

    nvcc --version
    nvidia-smi

-----------------------------------------------------------
ü¶ô 3. CLONAR LLAMA.CPP
-----------------------------------------------------------

    git clone https://github.com/ggerganov/llama.cpp.git
    cd llama.cpp

-----------------------------------------------------------
üõ†Ô∏è 4. COMPILAR LLAMA.CPP CON CUDA
-----------------------------------------------------------

Usando Make (recomendado):

    make clean
    make LLAMA_CUBLAS=1 -j$(nproc)

Alternativa usando CMake:

    mkdir build
    cd build
    cmake .. -DLLAMA_CUBLAS=ON
    cmake --build . --config Release -j$(nproc)

-----------------------------------------------------------
üìÅ 5. BINARIOS GENERADOS
-----------------------------------------------------------

Una vez compilado, encontrar√°s binarios como:

- llama-server
- llama-cli

Puedes copiarlos a la ra√≠z del proyecto
o a√±adir la carpeta al PATH del sistema.

-----------------------------------------------------------
‚úÖ 6. VERIFICACI√ìN
-----------------------------------------------------------

    ./llama-server --help

Si el comando responde correctamente,
llama.cpp est√° listo con soporte CUDA.

-----------------------------------------------------------
üöÄ 7. EJECUTAR EL SERVIDOR DE LLAMA.CPP
-----------------------------------------------------------

Una vez tengas el modelo en modelos/, inicia el servidor:

    ./llama-server \
        -m modelos/tu_modelo.gguf \
        -c 4096 \
        --port 8081

Esto levantar√° el servidor en:

    http://localhost:8081

-----------------------------------------------------------
üß© 8. EJECUTAR EL BACKEND FLASK
-----------------------------------------------------------

Desde la carpeta backend/:

    python app.py

El backend se iniciar√° en:

    http://localhost:8000

-----------------------------------------------------------
üí¨ 9. USAR EL CHAT
-----------------------------------------------------------

Abre en tu navegador:

    http://localhost:8000

La interfaz permite:

- Enviar mensajes
- Ver historial
- Recibir respuestas del modelo local

-----------------------------------------------------------
üß† 10. C√ìMO FUNCIONA EL BACKEND
-----------------------------------------------------------

El backend:

1. Recibe el mensaje del usuario
2. Lo a√±ade al historial
3. Construye un prompt estilo:
```
<s>[INST] mensaje [/INST]
```
4. Env√≠a el prompt al servidor llama.cpp
5. Recibe la respuesta
6. La devuelve al frontend

Endpoint principal:

    POST /v1/completions

-----------------------------------------------------------
üóÇÔ∏è 11. NOTAS IMPORTANTES
-----------------------------------------------------------

- El historial se guarda en memoria (no persistente)
- Si reinicias Flask, el historial se borra
- Puedes modificar el formato del prompt seg√∫n el modelo
- Proyecto pensado para uso local, no producci√≥n

-----------------------------------------------------------
üõ†Ô∏è 12. MEJORAS FUTURAS
-----------------------------------------------------------

- Persistencia del historial
- Soporte para m√∫ltiples modelos
- Configuraci√≥n desde interfaz
- Streaming de tokens
- Descarga autom√°tica desde Hugging Face

-----------------------------------------------------------
üìú 13. LICENCIA
-----------------------------------------------------------

Este proyecto es libre para uso personal y educativo.

===========================================================
 FIN DEL README
===========================================================

